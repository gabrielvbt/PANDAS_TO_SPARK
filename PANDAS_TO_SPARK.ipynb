{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"D:/spark-3.5.5-bin-hadoop3\"\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local[*]').getOrCreate()\n",
    "\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PANDAS = pd.read_csv('TESTE.csv', sep=';')\n",
    "SPARK = spark.read.csv('TESTE.csv', sep=';', inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBS: os Displays abaixo são apenas para mostrar todos os prints no mesmo output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O PRINT DO DATAFRAME DO SPARK É TOTALMENTE DIFERENTE, PORÉM ELE POSSUI UMA FUNÇÃO ESPECIFICA PARA FICAR COMO O DF DO PANDAS:\n",
    "display(PANDAS)\n",
    "SPARK.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(PANDAS.head(5))\n",
    "SPARK.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PANDAS.rename(columns={'TESTE': 'COLUNA_1'}, inplace=True)\n",
    "SPARK.withColumRenamed('TESTE', 'COLUNA_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(PANDAS.info)\n",
    "SPARK.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType, StringType\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "PANDAS['TESTE2'] = PANDAS['TESTE2'].str.replace(',', '.')\n",
    "SPARK = SPARK.withColumn('TESTE2', f.regexp_replace('TESTE2', ',', '.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PANDAS['TESTE2'] = PANDAS['TESTE2'].astype(float)\n",
    "SPARK = SPARK.withColumn('TESTE2', SPARK['TESTE2'].cast(DoubleType))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
