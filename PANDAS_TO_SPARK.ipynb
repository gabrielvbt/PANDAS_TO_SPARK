{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"D:/spark-3.5.5-bin-hadoop3\"\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local[*]').getOrCreate()\n",
    "\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PANDAS = pd.DataFrame([])\n",
    "SPARK = spark.createDataFrame([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PANDAS = pd.read_csv('TESTE.csv', sep=';')\n",
    "SPARK = spark.read.csv('TESTE.csv', sep=';', inferSchema=True)\n",
    "PANDAS_2 = pd.read_csv('TESTE_2.csv', sep=';')\n",
    "SPARK_2 = spark.read.csv('TESTE_2.csv', sep=';', inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBS: os Displays abaixo são apenas para mostrar todos os prints no mesmo output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O PRINT DO DATAFRAME DO SPARK É TOTALMENTE DIFERENTE, PORÉM ELE POSSUI UMA FUNÇÃO ESPECIFICA PARA FICAR COMO O DF DO PANDAS:\n",
    "display(PANDAS)\n",
    "SPARK.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(PANDAS.head(5))\n",
    "SPARK.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PANDAS.rename(columns={'TESTE': 'COLUNA_1'}, inplace=True)\n",
    "SPARK.withColumRenamed('TESTE', 'COLUNA_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(PANDAS.info)\n",
    "SPARK.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType, StringType\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "PANDAS['COLUNA_2'] = PANDAS['COLUNA_2'].str.replace(',', '.')\n",
    "SPARK = SPARK.withColumn('COLUNA_2', f.regexp_replace('COLUNA_2', ',', '.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PANDAS['COLUNA_2'] = PANDAS['COLUNA_2'].astype(float)\n",
    "SPARK = SPARK.withColumn('COLUNA_2', SPARK['COLUNA_2'].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASSUMINDO QUE O CAMPO DE DATA ESTA COMO INT/LONG, ETC EX 20200924 -> 24/09/2020\n",
    "PANDAS['COLUNA_3'] = pd.to_datetime(PANDAS['COLUNA_3'], dayfirst=True, errors='coerce')\n",
    "SPARK = SPARK.withColumn('COLUNA_3', f.to_date(SPARK.COLUNA_3.cast(StringType()), 'yyyMMdd'))#.withColum ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEMELHANTE AO SELECT SQL\n",
    "PANDAS[['COLUNA_1, COLUNA_2, COLUNA_3']]\n",
    "SPARK.select('*').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O DADO DEVE ESTAR EM FORMATO DATE\n",
    "PANDAS['COLUNA_4'] = PANDAS['COLUNA_3'].dt.year\n",
    "SPARK.select(f.year('COLUNA_3').alias('COLUNA_4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PANDAS[(pd.isnull(PANDAS['COLUNA_3'])) | (pd.isnull(PANDAS['COLUNA_4']))]\n",
    "SPARK.select(f.when(f.isnull('COLUNA_3')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELE LEVA EM CONSIDERAÇÃO A TIPAGEM \n",
    "PANDAS.fillna(0)\n",
    "SPARK.na.fill(0)\n",
    "\n",
    "# PARA NONE:\n",
    "PANDAS.fillna('-')\n",
    "SPARK.na.fill('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PANDAS.sort_values(by='COLUNA_4', ascending=False)\n",
    "SPARK.select('*').orderBy('COLUNA_4', ascending=False)\n",
    "\n",
    "PANDAS.sort_values(by=['COLUNA_4', 'COLUNA_3'], ascending=[False, False])\n",
    "SPARK.select('*').orderBy(['COLUNA_4', 'COLUNA_3'], ascending=[False, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WHERE E FILTER SÃO EQUIVALENTES NO SPARK\n",
    "PANDAS[PANDAS['COLUNA_3'] == 50]\n",
    "SPARK.where('COLUNA_3 == 50')\n",
    "\n",
    "PANDAS[(PANDAS['COLUNA_3'] == 50) | (PANDAS['COLUNA_3'] == 40)]\n",
    "SPARK.where('COLUNA_3 == 50').where('COLUNA_3 == 40')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PANDAS[(PANDAS[\"COLUNA_1\"].str.startswith(\"GABRIEL\")) & (PANDAS[\"COLUNA_1\"].str.endswith(\"TALIETTA\"))]\n",
    "SPARK.filter(COLUNA_1.startwith('GABRIEL')).filter(COLUNA_1.endswith('TALIETTA'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PANDAS[PANDAS['COLUNA_1'].str.contains('TESTE', case=False)]\n",
    "SPARK.where(f.upper(SPARK['COLUNA_1']).like('%TESTE%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PANDAS[PANDAS['COLUNA_4'] >= 2010]['COLUNA_3'].value_counts().sort_values(by='COLUNA_4')\n",
    "SPARK.select('COLUNA_3').where('COLUNA_4 >= 2010').groupBy('COLUNA_4').count().orderBy('COLUNA_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PANDAS.groupby('COLUNA_5').agg({'COLUNA_6': ['mean'], \n",
    "                                'COLUNA_7': ['count']})\n",
    "SPARK.select('*').groupBy('COLUNA_5').agg(f.avg('COLUNA_5').alias('COLUNA_6'),\n",
    "                                          f.count('COLUNA_5').alias('COLUNA_7'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PANDAS.describe()\n",
    "SPARK.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBS: NORMALMENTE UTILIZAMOS LEFT, POREM EXISTEM OUTRAS FORMAS DE MERGE/JOIN: 'inner', 'right', 'outer', ENTRE OUTROS\n",
    "PANDAS.merge(PANDAS_2, on='COLUNA_1', how='left')\n",
    "SPARK.join(SPARK_2, 'COLUNA_1', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([PANDAS, PANDAS_2], ignore_index=True)\n",
    "SPARK.union(SPARK_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandassql import sqldf\n",
    "sqldf(\"SELECT * FROM PANDAS\", locals())\n",
    "\n",
    "SPARK.CreateOrReplaceTempView(\"SPARK_TEMP\")\n",
    "spark.sql(\"SELECT * FROM SPARK_VIEW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A FORMA ABAIXO DE SPARK AUTOMATICAMENTE PARTICIONA O ARQUIVO\n",
    "# obs: não é possivel exportar os arquivos para a pasta local com spark, \n",
    "#      como fazemos no pandas, então, no minimo temos que criar uma pasta dentro da pasta local\n",
    "PANDAS.to_csv('PANDAS_TO_SPARK.csv', sep=';' index=False)\n",
    "SPARK.writer.csv(path='./PANDAS_TO_SPARK_CSV', mode='overwrite', sep=';', header=True)\n",
    "\n",
    "PANDAS.to_parquet('PANDAS_TO_SPARK.parquet', index=False)\n",
    "SPARK.writer.parquet(path='./PANDAS_TO_SPARK_PARQUET'mode='overwrite')\n",
    "\n",
    "# PARA CRIAR UM ARQUIVO ÚNICO COM SPARK\n",
    "SPARK.coalesce(1).write.csv(path='PANDAS_TO_SPARK_N_PARTICIONADO', mode='overwrite', sep=';', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AO FINALIZAR UM NOTEBOOK É NECESSÁRIO FECHAR A SEÇÃO SPARK COM:\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
